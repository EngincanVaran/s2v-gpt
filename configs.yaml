global_configs:
  exp_num: 1

training_configs:
  # Basic training parameters
  batch_size: 8192 # Number of samples processed before the model is updated
  epochs: 2
  eval_interval: 100
  learning_rate: 0.0003

model_configs:
  # Model configuration
  vocab_size: 4096  # The size of the vocabulary
  n_layer: 6  # The number of transformer layers
  n_head: 6  # The number of attention heads in each transformer layer
  n_embd: 240  # The dimensionality of embeddings and hidden layers
  block_size: 32  # The maximum length of input sequences
  # Regularization and optimization
  dropout: 0.2  # Dropout rate to prevent overfitting
  bias: true  # Whether to include bias terms in linear layers
