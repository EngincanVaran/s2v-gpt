# Basic training parameters
batch_size: 512 # Number of samples processed before the model is updated
num_epochs: 1
learning_rate: 0.001

# Model configuration
vocab_size: 4096  # The size of the vocabulary
n_layer: 8  # The number of transformer layers
n_head: 4  # The number of attention heads in each transformer layer
n_embd: 256  # The dimensionality of embeddings and hidden layers
block_size: 64  # The maximum length of input sequences

# Regularization and optimization
dropout: 0.1  # Dropout rate to prevent overfitting
bias: true  # Whether to include bias terms in linear layers
