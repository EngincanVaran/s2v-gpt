global_configs:
  exp_num: 1

prediction_configs:
  max_workers: 20
  block_size: 32
  batch_size: 50000
  top_k: 11

evaluation_configs:
  window_size: 50000
  suspicious_threshold: 0.7
  lag: 50000

training_configs:
  batch_size: 1024  # Larger batch size since the model is smaller
  epochs: 3
  eval_interval: 100
  learning_rate: 1e-3  # Lower learning rate for stability
  milestones: [1, 2]
  gamma: 0.1

model_configs:
  vocab_size: 4096
  n_layer: 4  # Fewer transformer layers
  n_head: 4 # Fewer attention heads
  n_embd: 128 # Smaller embedding size
  block_size: 64  # Smaller context window
  dropout: 0  # Add dropout for better generalization
  bias: true
